{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Web Scraping\n",
    "\n",
    "Encontrar√°s en este cuaderno algunos ejercicios de web scraping para practicar tus habilidades de scraping usando `requests` y `Beautiful Soup`.\n",
    "\n",
    "**Consejos:**\n",
    "\n",
    "- Verifica el [c√≥digo de estado de la respuesta](https://http.cat/) para cada solicitud para asegurarte de haber obtenido el contenido previsto.\n",
    "- Observa el c√≥digo HTML en cada solicitud para entender el tipo de informaci√≥n que est√°s obteniendo y su formato.\n",
    "- Busca patrones en el texto de respuesta para extraer los datos/informaci√≥n solicitados en cada pregunta.\n",
    "- Visita cada URL y echa un vistazo a su fuente a trav√©s de Chrome DevTools. Necesitar√°s identificar las etiquetas HTML, nombres de clases especiales, etc., utilizados para el contenido HTML que se espera extraer.\n",
    "- Revisa los selectores CSS.\n",
    "\n",
    "### Recursos √ötiles\n",
    "- Documentaci√≥n de la [biblioteca Requests](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Doc de Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Lista de c√≥digos de estado HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [Conceptos b√°sicos de HTML](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [Conceptos b√°sicos de CSS](https://www.cssbasics.com/#page_start)\n",
    "\n",
    "#### Primero que todo, reuniendo nuestras herramientas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Nuevamente, recuerda limitar tu salida antes de la entrega para que tu c√≥digo no se pierda en la salida.**\n",
    "\n",
    "#### Desaf√≠o 1 - Descargar, analizar (usando BeautifulSoup) e imprimir el contenido de la p√°gina de Desarrolladores en Tendencia de GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")\n",
    "developers = []\n",
    "for developer in soup.select(\"h1:nth-child(1).h3 > a:nth-child(1)\"):\n",
    "    developers.append(developer.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestra los nombres de los desarrolladores en tendencia recuperados en el paso anterior.\n",
    "\n",
    "Tu salida debe ser una lista de Python con los nombres de los desarrolladores. Cada nombre no debe contener ninguna etiqueta HTML.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Descubre la etiqueta HTML y los nombres de clase usados para los nombres de los desarrolladores. Puedes lograr esto usando Chrome DevTools.\n",
    "\n",
    "1. Usa BeautifulSoup para extraer todos los elementos HTML que contienen los nombres de los desarrolladores.\n",
    "\n",
    "1. Utiliza t√©cnicas de manipulaci√≥n de cadenas para reemplazar espacios en blanco y saltos de l√≠nea (es decir, `\\n`) en el *texto* de cada elemento HTML. Usa una lista para almacenar los nombres limpios.\n",
    "\n",
    "1. Imprime la lista de nombres.\n",
    "\n",
    "Tu salida deber√≠a lucir como abajo (con nombres diferentes):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (Á•ûÊ•ΩÂùÇË¶ö„ÄÖ)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain4j',\n",
       " 'Natik Gadzhi',\n",
       " 'Vik Paruchuri',\n",
       " 'jxxghp',\n",
       " 'Marcus Schiesser',\n",
       " 'Evan Wallace',\n",
       " 'Yair Morgenstern',\n",
       " 'Norman Maurer',\n",
       " 'Martin von Zweigbergk',\n",
       " 'Emil Ernerfeldt',\n",
       " 'Keren Zhou',\n",
       " 'Riad Benguella',\n",
       " 'Xuan Son Nguyen',\n",
       " '‰∏âÂí≤ÈõÖ ¬∑ Misaki Masa',\n",
       " 'Massimiliano Pippi',\n",
       " 'Steve Macenski',\n",
       " 'john verzani',\n",
       " 'Sebastian Raschka',\n",
       " 'Mohammed Abu Aboud',\n",
       " 'Azure SDK Bot',\n",
       " 'Robin Malfait',\n",
       " 'JJ Kasper',\n",
       " 'Henry Heng',\n",
       " 'Miguel √Ångel Dur√°n',\n",
       " 'Matthias Seitz']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 2 - Mostrar los repositorios de Python en tendencia en GitHub\n",
    "\n",
    "Los pasos para resolver este problema son similares al anterior, excepto que necesitas encontrar los nombres de los repositorios en lugar de los nombres de los desarrolladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "datos2 = requests.get(f\"{url2}\").text\n",
    "soup2 = BeautifulSoup(datos2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Codium-ai/cover-agent',\n",
       " 'khoj-ai/khoj',\n",
       " 'searxng/searxng',\n",
       " 'OpenBMB/MiniCPM-V',\n",
       " 'W01fh4cker/CVE-2024-21683-RCE',\n",
       " 'google-research/omniglue',\n",
       " 'ungoogled-software/ungoogled-chromium',\n",
       " 'dataelement/bisheng',\n",
       " 'nvaccess/nvda',\n",
       " 'infiniflow/ragflow',\n",
       " 'epic-open-source/seismometer',\n",
       " 'home-assistant/core',\n",
       " 'huggingface/accelerate',\n",
       " 'Avaiga/taipy',\n",
       " 'apple/corenet',\n",
       " 'lavague-ai/LaVague',\n",
       " 'assafelovic/gpt-researcher',\n",
       " 'Acly/krita-ai-diffusion',\n",
       " 'Azure/azure-sdk-for-python',\n",
       " 'unslothai/unsloth',\n",
       " 'TagStudioDev/TagStudio',\n",
       " 'pathwaycom/pathway',\n",
       " 'geekcomputers/Python',\n",
       " 'jackfrued/Python-100-Days',\n",
       " 'TobikoData/sqlmesh']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "repos = soup2.find_all('h2', class_='h3 lh-condensed')\n",
    "lista_repositorios=[]\n",
    "for repo in repos:\n",
    "    repo_nombre = repo.a['href'].strip('/')\n",
    "    lista_repositorios.append(repo_nombre)\n",
    "lista_repositorios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 3 - Mostrar todos los enlaces de im√°genes de la p√°gina de Wikipedia de Walt Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Nuvola_apps_kaboodle.svg/16px-Nuvola_apps_kaboodle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/23px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/26px-Wikisource-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Mickey_Mouse_colored_%28head%29.svg/20px-Mickey_Mouse_colored_%28head%29.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "content = soup3.find('div', {\"id\": \"bodyContent\"})\n",
    "images = content.find_all(\"img\")\n",
    "urls = []\n",
    "for image in images:\n",
    "    urls.append(image.get(\"src\"))\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 4 - Recuperar todos los enlaces a p√°ginas en Wikipedia que se refieren a alg√∫n tipo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://az.wikipedia.org/wiki/Python_(d%C9%99qiql%C9%99%C5%9Fdirm%C9%99)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/Python_(codename)',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "python_links = soup4.find_all(\"a\", title=lambda title: title and \"Python\" in title)\n",
    "\n",
    "python_pages=[]\n",
    "for link in python_links:\n",
    "    python_pages.append(link.get(\"href\"))\n",
    "python_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 5 - N√∫mero de T√≠tulos que han cambiado en el C√≥digo de los Estados Unidos desde su √∫ltimo punto de lanzamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "html = requests.get(url).text\n",
    "soup5 = BeautifulSoup(html, \"html.parser\")\n",
    "title_divs = soup5.find_all(\"div\", class_=\"usctitlechanged\")\n",
    "len(title_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 6 - Una lista de Python con los diez nombres m√°s buscados por el FBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = \"https://es.wikipedia.org/wiki/Los_diez_fugitivos_m%C3%A1s_buscados_por_el_FBI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = requests.get(f\"{url7}\").text\n",
    "soup7 = BeautifulSoup(wanted, 'html.parser')\n",
    "wantedtag = soup7.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alexis Flores', '2 de junio de 2007', '487'],\n",
       " ['Bhadreshkumar Chetanbhai Patel', '18 de abril de 2017', '514'],\n",
       " ['Alejandro Castillo', '25 de septiembre de 2017', '516'],\n",
       " ['Arnoldo Jimenez', '8 de mayo de 2019', '522'],\n",
       " ['Yulan Adonay Archaga Carias', '3 de noviembre de 2021', '526'],\n",
       " ['Ruja Ignatova', '30 de junio de 2022', '527'],\n",
       " ['Omar Alexander Cardenas', '20 de julio de 2022', '528'],\n",
       " ['Wilver Villegas-Palomino', '14 de abril de 2023', '530'],\n",
       " ['Donald Eugene Fields II', '25 de mayo de 2023', '531'],\n",
       " [\"Vitel'Homme Innocent\", '15 de noviembre de 2023', '532']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "wantedtable = soup7.find(\"tbody\")\n",
    "wantedrows = wantedtable.find_all(\"div\", class_=\"center\")\n",
    "wantes = []\n",
    "wanted = requests.get(f\"{url7}\").text\n",
    "soup7 = BeautifulSoup(wanted, 'html.parser')\n",
    "wantedtag = soup7.find_all('h3')\n",
    "\n",
    "for i in range(0, len(wantedrows), 3):\n",
    "    name = wantedrows[i].get_text()\n",
    "    date = wantedrows[i + 1].get_text()\n",
    "    seq_number = wantedrows[i + 2].get_text()\n",
    "    wantes.append([name, date, seq_number])\n",
    "wantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 7 - Listar todos los nombres de idiomas y el n√∫mero de art√≠culos relacionados en el orden en que aparecen en wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url8}\").text\n",
    "soup8 = BeautifulSoup(languages, 'html.parser')\n",
    "langlist = soup8.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 6,796,000+ articles\n",
      "√ê¬†√ë¬É√ë¬Å√ë¬Å√ê¬∫√ê¬∏√ê¬π: 1√Ç¬†969√Ç¬†000+ √ë¬Å√ë¬Ç√ê¬∞√ë¬Ç√ê¬µ√ê¬π\n",
      "√¶¬ó¬•√¶¬ú¬¨√®¬™¬û: 1,407,000+ √®¬®¬ò√§¬∫¬ã\n",
      "Espa√É¬±ol: 1.938.000+ art√É¬≠culos\n",
      "Deutsch: 2.891.000+ Artikel\n",
      "Fran√É¬ßais: 2√¢¬Ä¬Ø598√¢¬Ä¬Ø000+ articles\n",
      "Italiano: 1.853.000+ voci\n",
      "√§¬∏¬≠√¶¬ñ¬á: 1,409,000+ √¶¬ù¬°√ß¬õ¬Æ / √¶¬¢¬ù√ß¬õ¬Æ\n",
      "√ô¬Å√ò¬ß√ò¬±√ò¬≥√õ¬å: √õ¬π√õ¬π√õ¬µ√ô¬¨√õ¬∞√õ¬∞√õ¬∞+ √ô¬Ö",
      "√ô¬Ç√ò¬ß√ô¬Ñ√ô¬á\n",
      "Portugu√É¬™s: 1.120.000+ artigos\n"
     ]
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "language_list = []\n",
    "\n",
    "for language in langlist:\n",
    "    lnombres = language.find(\"strong\").text.strip()\n",
    "    larticulos = language.find(\"small\").text.strip()\n",
    "    language_list.append((lnombres,larticulos))\n",
    "for i,j in language_list:\n",
    "    print(f'{i}: {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 8 - Una lista con los diferentes tipos de conjuntos de datos disponibles en data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url82 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url82}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cookies to collect information',\n",
       " 'View cookies',\n",
       " 'change your cookie settings',\n",
       " 'feedback',\n",
       " 'Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "datasets = soup8.find_all('a',{'class':\"govuk-link\"})\n",
    "lista = [tag.getText()for tag in datasets]\n",
    "lista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 9 - Los 10 idiomas con m√°s hablantes nativos almacenados en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mandarin Chinese',\n",
       " 'Spanish',\n",
       " 'English',\n",
       " 'Arabic',\n",
       " 'Hindi',\n",
       " 'Bengali',\n",
       " 'Portuguese',\n",
       " 'Russian',\n",
       " 'Japanese',\n",
       " 'Western Punjabi',\n",
       " 'Javanese']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "idiomas = soup9.findAll(\"tbody\")\n",
    "table = soup9.find(\"table\", {\"class\":\"wikitable sortable\"})\n",
    "table\n",
    "\n",
    "languages = table.find_all(\"a\", title=True)\n",
    "countries = []\n",
    "for i in languages:\n",
    "    countries.append(i.text.strip())\n",
    "    \n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subiendo el nivel\n",
    "#### Desaf√≠o 10 - La informaci√≥n de los 20 √∫ltimos terremotos (fecha, hora, latitud, longitud y nombre de la regi√≥n) por el EMSC como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "url7 = \"https://www.emsc-csem.org/#2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠\n",
    "data = [] #this is going to be a list of lists, each element will be the info of a earthquake\n",
    "table = soup10.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 11 - Datos del Top 250 de IMDB (nombre de la pel√≠cula, lanzamiento inicial, nombre del director y estrellas) como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url11 = 'https://www.imdb.com/chart/top'\n",
    "pelis = requests.get(url11)\n",
    "soup = BeautifulSoup(pelis.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 12 - Nombre de la pel√≠cula, a√±o y un breve resumen de las 10 pel√≠culas aleatorias top (IMDB) como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(soup.find_all(\"span\", {\"class\": \"secondaryInfo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 13 - Encontrar el reporte meteorol√≥gico en vivo (temperatura, velocidad del viento, descripci√≥n y clima) de una ciudad dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(city):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desaf√≠o 14 - Nombre del libro, precio y disponibilidad de stock como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url14 = 'http://books.toscrape.com/'\n",
    "books = requests.get(url11)\n",
    "soup = BeautifulSoup(books.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libros = list(soup.select('h1.h3 a[tittle]'))\n",
    "#nombres = soup.select('h1.h3 a[tittle]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitates tu output? Gracias! üôÇ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
